<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="keywords" content="Diffusion-based Data Augmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Diversify Your Vision Datasets with Automatic Diffusion-based Augmentation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/custom.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
 

<section class="hero space-background has-text-black">
  <div class="hero-body space-background-overlay">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title has-text-black">Diversify Your Vision Datasets with Automatic Diffusion-based Augmentation</span></h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.lisabdunlap.com">Lisa Dunlap</a>,</span>
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/alyssa-umino-b897531a2/">Alyssa Umino</a>,</span>
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/pariszhang11/">Han Zhang</a>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/stephenyangjz">Jiezhi Yang</a>,
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~jegonzal/">Joseph E. Gonzalez</a>,
            </span>
            <span class="author-block">
              <a href="http://people.eecs.berkeley.edu/~trevor">Trevor Darrell</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">UC Berkeley</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2305.16289"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/lisadunlap/ALIA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <!-- <span class="icon">
                     <i class="bi bi-medium"></i>
                  </span> -->
                  <span>Blog (Coming Soon)</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- center -->
        <h4 class="is-centered has-text-centered">Takeaway 1: use captioning models and LLM's to extract task-agnostic concepts from training data, then edit training images using these text concepts to diversify your training set. Takeaway 2: Image-editing is preferred over pure generation if your task is specialized.</h4>
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Many fine-grained classification tasks, like rare animal identification, have limited training 
            data and consequently classifiers trained on these datasets often fail to generalize to 
            variations in the domain like changes in weather or location. As such, we explore how natural 
            language descriptions of the domains seen in training data can be used with large vision models
            trained on diverse pretraining datasets to generate useful variations of the training data. 
            We introduce ALIA (Automated Language-guided Image Augmentation), a method which utilizes 
            large vision and language models to automatically generate natural language descriptions of a dataset's
            domains and augment the training data via language-guided image editing. To maintain data integrity, 
            a model trained on the original dataset filters out minimal image edits and those which corrupt class-relevant information. 
            The resulting dataset is visually consistent with the original training data and offers significantly enhanced diversity. 
            On fine-grained and cluttered datasets for classification and detection, ALIA surpasses traditional data augmentation 
            and text-to-image generated data by up to 15%, often even outperforming equivalent additions of real data.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
  <figure class="image" id="fig-method">
  <img src="./assets/teaser.png" class="image-spacing, center" style="width: 700px">
  <!-- <figcaption class="center image-caption" style="width: 750px">Example augmentations using text-to-images generation, traditional data augmentation methods, and our method, Automated Language-guided Image Editing (ALIA) on CUB. Images generated by ALIA retain task-relevant information while providing more domain diversity as specified by the prompts. Within the prompts, { } indicates the specific class name.</figcaption> -->
  </figure>
  </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- A -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">ALIA Approach</h2>
        <div class="content has-text-justified">

        <!-- <p>We focus on how to utilize pretrained vision and language models for image captioning and generation as a <em>translation</em> layer between task-specific image data and task-agnostic natural language descriptions of domains. These high-level domain descriptions, well-represented by image generation models like Stable Diffusion, can be used for <i>language-guided image editing</i> of the specialized training data. This approach produces visually consistent images with the training data, introduces variations in task-agnostic domains, and preserves task-relevant information present in the original image.</p> -->


        <!-- <p>ALIA does not require fine-tuning of the image captioning or image generation model, nor does it rely on user-supplied prompts. We show that our generated data leads to improved performance for domain generalization, fine-grained classification, and addressing bias.</p> -->


        </div>
        <figure class="image" id="fig-method">
	  	  <img src="./assets/approach.png" class="center" style="width: 750px">
         <figcaption class="center image-caption"  style="width: 750px"><strong>ALIA.</strong> Given a specialized dataset, we caption all images in our dataset using a pretrained captioning model, and feed these captions into a large language model to summarize them into a small (<10) set of natural language descriptions.  -->
    Utilizing these descriptions, we perform text-guided image augmentation via a pretrained text-to-image model, thereby generating training images that align with the described settings. Finally, we apply two filtering techniques: a CLIP-based semantic filter to eliminate obvious edit failures, and a confidence-based filter that removes more subtle failures by filtering edits confidently predicted by a classifier trained on the original dataset.</figcaption>
	  	</figure>
      </div>
    </div>
</section>
    <!--/ A -->
<section class="section">
    <!-- B -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">The Good Part (Results)</h2>
        <!-- <div class="content has-text-justified">
          <p>The prompts generated by <i>ALIA</i>:
            <ol>
              <li><i>"a camera trap photo of a { } in a grassy field with trees and bushes."</i></li>
              <li><i>"a camera trap photo of a { } in a forest in the dark."</i></li>
              <li><i>"a camera trap photo of a { } near a large body of water in the middle of a field."</i></li>
              <li><i>"a camera trap photo of a { } walking on a dirt trail with twigs and branches."</i></li>
            </ol>

        </div> -->
          <figure class="image column">
            <img src="./assets/wilds.png" class="center" style="width: 750px">
            <!-- <figcaption class="center image-caption"  style="width: 750px"><strong>iWildCam.</strong> Original training images and the corresponding data generated from ALIA based on the generated prompts (left) and the macro F1-scores for adding in generated data from ALIA, Txt2Img, real data, CutMix augmentation, and RandAug augmentation (right). Not only is ALIA able to outperform the augmentation baseles and text-to-image generated data, it also outperforms the addition of real data!</figcaption> -->
            <!-- <figcaption class="center image-caption"  style="width: 750px"><strong>iWildCam.</strong> Evaluation on domain generalization dataset, where locations in the testset are not present in the training set. Not only is ALIA able to outperform the augmentation baseles and text-to-image generated data, it also outperforms the addition of real data!</figcaption> -->
          </figure>
          <h2 class="title is-size-6">Domain Generalization [iWildCam]</h2>
      </div>
    </div>
    <!--/ B -->
    <!-- C -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
       
        <!-- <div class="content has-text-justified">
          <p>The prompts generated by <i>ALIA</i>:</p>
          <ol>
            <li><i>"a photo of a { } bird interacting with flowers."</i></li>
            <li><i>"a photo of a { } bird standing by the waters edge."</i></li>
            <li><i>"a photo of a { } bird perched on a fence."</i></li>
            <li><i>"a photo of a { } bird standing on a rock."</i></li>
            <li><i>"a photo of a { } bird perched on a branch."</i></li>
            <li><i>"a photo of a { } bird flying near a tree, sky as the backdrop."</i></li>
            <li><i>"a photo of a { } bird perched on a birdfeeder."</i></li>
          </ol>
        </div> -->
      
          <figure class="image column">
            <img src="./assets/cub.png" class="center" style="width: 750px">
            <!-- <figcaption class="center image-caption"  style="width: 750px"><strong>CUB.</strong> Evaluation on fine-grained bird classification with the same train and test distribution. ALIA is able to outperform all data augmentation techniques. </figcaption> -->
          </figure>
          <h2 class="title is-size-6">Fine-Grained Classification [CUB]</h2>
      </div>
    </div>
    <!--/ C -->
    <!-- D -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        
        <!-- <div class="content has-text-justified">
                  
        <p>The prompts generated by <i>ALIA</i>:</p>
        <ol>
          <li><i>"a photo of a { } airplane on the airport tarmac, surrounded by buildings and other infrastructure."</i></li>
          <li><i>"a photo of a { } airplane parked on the runway, grass and trees in the backdrop."</i></li>
          <li><i>"a photo of a { } airplane parked on a runway, with a vast desert expanding in the background."</i></li>
          <li><i>"a photo of a { } airplane a photo of a { } airplane with red and white colors, landing gear down, against a backdrop of a bustling cityscape."</i></li>
          <li><i>"a photo of a { } airplane in mid-flight, landing gear deployed against a clear sky."</i></li>
        </ol>
        </div> -->
      
       
          <figure class="image column">
            <img src="./assets/planes.png" class="center" style="width: 750px">
            <!-- <figcaption class="center image-caption"  style="width: 750px"><strong>Airbus vs Booeing.</strong>Results on dataset of Airbus vs Boeing planes where Airbus planes are never seen in the grass and Being planes are never seen on the road during training. ALIA is able to generate useful domai</figcaption> -->
          </figure>
          <h2 class="title is-size-6">Contextual Bias [Airbus on the road VS Boeing on grass]</h2>

      </div>
    </div>
    <!--/ D -->
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{dunlap2023alia,
  author    = {Dunlap, Lisa and Umino, Alyssa and Zhang, Han and Yang, Jiezhi and Gonzalez, Joseph and Darrell, Trevor},
  title     = {Diversify Your Vision Datasets with Automatic
    Diffusion-based Augmentation},
  journal   = {Conference on Neural Information Processing Systems},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a 
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
              Creative Commons Attribution-ShareAlike 4.0 International License
            </a>. 
            The website template is from the 
            <a href="https://github.com/nerfies/nerfies.github.io">
              Nerfies
            </a> 
            project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
